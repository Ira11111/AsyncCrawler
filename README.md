# AsyncCrawler

### Описание
Это реализация классического приложение для асинхронных задач -- интернет краулера.

Принцип работы такой: на вход краулеру дается ссылка или ссылки.
Далее он запрашивает контент этих страниц, парсит их, достает оттуда все доступные ссылки и уже идет по ним. 
Иначе говоря, работает рекурсивно. Будем искать ссылки только на внешние ресурсы.

Приложение реализовано с использованием асинхронного python и модуля aiohttp. 

### Настройка и запуск

Для конфигурации работы краулера нужно отредактировать файл `conf.json` где будут указаны:

- MAX_DEPTH: максимальная глубина рекурсивного поиска
- OUTPUT_PATH: файл, в который будут записаны найденные ссылки на внешние источники
- START_URLS: перечень ссылок, с которых начинается поиск

---

Создайте виртуальное окружение и установите зависимости командой:
```bash
pip install -r requirements.txt
```

---

Для запуска нужно выполнить команду:
```bash
python app.py
```