# AsyncCrawler

### Описание
Это реализация классического приложение для асинхронных задач -- интернет краулера.

Принцип работы такой: на вход краулеру дается ссылка или ссылки.
Далее он запрашивает контент этих страниц, парсит их, достает оттуда все доступные ссылки и уже идет по ним. 
Иначе говоря, работает рекурсивно. Будем искать ссылки только на внешние ресурсы.

Приложение реализовано с использованием асинхронного python и модуля aiohttp. 

### Настройка и запуск

Для конфигурации работы краулера нужно создать файл .env где будут указаны:

- максимальная глубина рекурсивного поиска
- файл, в который будут записаны найденные ссылки на внешние источники
- перечень ссылок, с которых начинается поиск

В качестве примера можно ориентироваться на файл env.template

---

Создайте виртуальное окружение и установите зависимости командой:
```bash
pip install -r requirements.txt
```

---

Для запуска нужно выполнить команду:
```bash
python app.py
```